
# Computational kindness and virtue ethics

Avoiding excess computation has been a common conversation topic around
my house lately. I've found it useful to think about when asking people for things,
and also when making my own plans. This post is about ethics though.

## Utilitarianism

Basically, utilitarianism is the perfect ethical framework _when you have all the information_.
It is like optimal caching (the best caching strategy is to cache everything that you will
need in the future, and nothing else, obviously). But like optimal caching, that doesn't
mean that utilitarianism is terribly _useful_, since we aren't omniscient.

I am not a philosopher, so for me, defining utilitarianism as "the ethical framework where
you do whatever will have the best outcome" is sufficient, especially since I'm not concerned
with what cost function is used to define the "best outcome." In any case, to do
utilitarianism properly, you traverse every branch to whatever you define as its conclusion,
total up the _utils_ and then take the best option.

I used to really like utilitarianism, and felt like it made a lot of sense. It also made me
face a lot of uncomfortable ideas, as I suppose good philosophical reading should.
Now that I'm less arrogant than I was as a teenager, I don't like utilitarianism as much.
Now I like virtue ethics.


## Virtue Ethics

I'll flimsily define virtue ethics as "the ethical framework where you do things that you
have previously defined as virtuous, according to some set of virtues." Whatever set of virtues
are at play, to do virtue ethics, you look at the possibly actions and tally up their respective
virtuousness, and choose the best option. Unlike utilitarianism, it is not necessary to know
the outcomes of the actions. I like to think of this as making choices that you will still feel
were right, even if things don't turn out as expected.


## Computation

Using utilitarianism, we need to traverse all probable paths
(and the probable paths stemming from those nodes) until we reach nodes that
have clear enough values in utils for us to make a decision off of.
We are making predictions about what the state of the world will be after the
decision is made.

Using virtue ethics, we instead are trying to predict the future of how we
will feel about the decision. I think this is a much easier problem.

